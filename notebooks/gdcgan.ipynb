{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gdcgan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEqaAyV4iCqu",
        "colab_type": "code",
        "outputId": "f3d89fed-d392-4452-fc07-74e2b34679c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv3XwsZoifiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/My\\ Drive/giraffe/root.zip root.zip\n",
        "!cp /content/drive/My\\ Drive/giraffe/images.pkl images.pkl\n",
        "!unzip root.zip > ziplog.txt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csQy25FJjWVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aldCvh75WY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
        "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=1)  # b, 8, 2, 2\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(8, 3, 6, stride=2, padding=1),  # b, 1, 28, 28\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592iGSoPJfp6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1IooGWQVOef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size, scene_size):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.init_size = img_size // 4\n",
        "        self.l1 = nn.Sequential(nn.Linear(scene_size, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.l1(z)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwiJ1-yXceb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_scene = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5)),\n",
        "])\n",
        "\n",
        "class RenderDataset(Dataset):\n",
        "    \"\"\"Renderdataset.\"\"\"\n",
        "    def __init__(self, scenes_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            scenes_file (string): Path to the pkl file with scene description.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.scenes_file = pickle.load(open(scenes_file, 'rb'))\n",
        "        self.root_dir = root_dir\n",
        "        self.images = [image for image in os.listdir(self.root_dir) if image.endswith(('.png', '.jpg'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.images[idx])\n",
        "        image = Image.open(img_name)\n",
        "        scene = self.scenes_file[idx]\n",
        "        scene = torch.tensor(scene)\n",
        "\n",
        "        sample = {'image': image, 'scene': scene}\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform(sample['image'])\n",
        " \n",
        "        return sample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALk5bQAxiyhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_size = 64\n",
        "img_size = 128\n",
        "scene_size = 57\n",
        "\n",
        "train_dataset = RenderDataset('images.pkl','root/images', transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=b_size, shuffle=False, num_workers=4,\n",
        ")\n",
        "\n",
        "\n",
        "num_epochs = 300\n",
        "learning_rate = 1e-3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# model = autoencoder()\n",
        "model = Generator(img_size, scene_size)\n",
        "model.to(device)\n",
        "# Initialize weights\n",
        "model.apply(weights_init_normal)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), lr=learning_rate, \n",
        "    weight_decay=1e-5\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBSEE9PznI01",
        "colab_type": "code",
        "outputId": "2e27b208-e7d2-4514-b645-441e6e719117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    x = x.view(x.size(0), 3, img_size, img_size)\n",
        "    return x\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader:\n",
        "\n",
        "        render = data['image']\n",
        "        scene = data['scene']\n",
        "\n",
        "        # if epoch == 0:\n",
        "          # print(render[0])\n",
        "          # img = Image.fromarray(render[0])\n",
        "          # img.show()\n",
        "          # print(scene[0])\n",
        "\n",
        "\n",
        "        render = render.view(render.size(0), 3, img_size, img_size)\n",
        "        render = Variable(render).to(device)\n",
        "        scene = Variable(scene).to(device)\n",
        "        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(scene)\n",
        "\n",
        "        loss = criterion(output, render)\n",
        "        losses.append(loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.data))\n",
        "        pic = to_img(output.cpu().data)\n",
        "        save_image(pic, '/content/drive/My Drive/giraffe/train_dcgan/{}.png'.format(str(epoch+1)))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/giraffe/train/checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch [1/300], loss:0.2165\n",
            "epoch [11/300], loss:0.2170\n",
            "epoch [21/300], loss:0.2157\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}